{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENTS = [{\n",
    "        \"config\": \"DexYCB_HandMvNet.yaml\",\n",
    "        \"weights\":  \"../weights/dexycb/HandMvNet/lightning_logs/version_1191880/checkpoints/epoch=150-step=60702-val_mpjpe=5.974.ckpt\"\n",
    "    },{\n",
    "        \"config\": \"HO3D_HandMvNet_HR_wo_cam.yaml\",\n",
    "        \"weights\": \"../weights/ho3d/HandMvNet-HR_wo_cam/lightning_logs/version_1256682/checkpoints/epoch=20-step=5061-val_mpjpe=14.263.ckpt\",\n",
    "    },{\n",
    "        \"config\": \"MVHand_HandMvNet_HR_wo_cam.yaml\",\n",
    "        \"weights\": \"../weights/mvhand/HandMvNet-HR_wo_cam/lightning_logs/version_1259582/checkpoints/epoch=98-step=23760-val_mpjpe=1.763.ckpt\"\n",
    "    }\n",
    "]\n",
    "\n",
    "SELECTED_EXP = EXPERIMENTS[2]\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "os.environ[\"PYOPENGL_PLATFORM\"] = \"egl\"\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "sys.argv = [\"config.py\", \"--config\", f\"../configs/release/{SELECTED_EXP['config']}\"]\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import lightning as L\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from config import cfg\n",
    "\n",
    "from vis.visualizer import HandPoseVisualizer\n",
    "from vis.utils import reverse_transform\n",
    "\n",
    "from utils.camera import transform_joints_between_cameras, get_2d_joints_from_3d_joints, create_intrinsics_matrix\n",
    "\n",
    "from datasets.mvhand import MVHandDataModule\n",
    "from datasets.dexycb import DexYCBDataModule\n",
    "from datasets.ho3d import HO3DDataModule\n",
    "from datasets.utils import batch_joints_img_to_cropped_joints\n",
    "\n",
    "from models.handmvnet import HandMvNet as Model\n",
    "from models.joints_to_vertices import JointsToVertices\n",
    "\n",
    "\n",
    "cfg[\"data\"][\"batch_size\"] = 1\n",
    "cfg[\"data\"][\"num_workers\"] = 1\n",
    "# fix relative paths\n",
    "if not cfg[\"data\"][\"dataset_dir\"].startswith(\"../\"):\n",
    "    cfg[\"data\"][\"dataset_dir\"] = os.path.join(\"..\", cfg[\"data\"][\"dataset_dir\"])\n",
    "    cfg[\"data\"][\"mano_models_dir\"] = os.path.join(\"..\", cfg[\"data\"][\"mano_models_dir\"])\n",
    "    cfg[\"base_output_dir\"] = os.path.join(\"..\", cfg[\"base_output_dir\"])\n",
    "\n",
    "SKIP_EVERY = 1\n",
    "NUM_SAMPLES_TO_SAVE = 5\n",
    "SAVE_INDIVIDUAL = False\n",
    "SAVE_GT = False\n",
    "VIEW_ONLY = True\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# DEVICE = \"cpu\"\n",
    "OUTPUT_DIR = os.path.join(cfg[\"base_output_dir\"], \"infer\")\n",
    "\n",
    "print(json.dumps(cfg, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_legacy_version(state_dict):\n",
    "    \"\"\"\n",
    "    Check if the model is a legacy version based on mismatch between keys.\n",
    "    \"\"\"\n",
    "    # Define common mismatching keys for legacy models\n",
    "    legacy_keys = [\"pose_net.conv.0.weight\", \"sample_net.conv.0.weight\"]\n",
    "    for key in legacy_keys:\n",
    "        if key in state_dict:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def load_checkpoint_with_legacy_fix(checkpoint_path, model, device='cpu'):\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "    # Extract the state_dict from the checkpoint\n",
    "    state_dict = checkpoint['state_dict']\n",
    "\n",
    "    # Check if it's a legacy version by inspecting mismatched keys\n",
    "    if is_legacy_version(state_dict):\n",
    "        print(\"[warning] Legacy version detected. Remapping keys...\")\n",
    "        # Create a new state_dict with remapped keys\n",
    "        new_state_dict = OrderedDict()\n",
    "        for old_key, value in state_dict.items():\n",
    "            # Replace old keys with new ones\n",
    "            new_key = old_key.replace('pose_net.conv.', 'pose_net.') \\\n",
    "                             .replace('sample_net.', 'sample_nets.0.')\n",
    "            # Add the remapped key-value pair\n",
    "            new_state_dict[new_key] = value\n",
    "        # Load the remapped state_dict into the model\n",
    "        model.load_state_dict(new_state_dict, strict=True)\n",
    "        print(\"[info] legacy model loaded successfully.\")\n",
    "    else:\n",
    "        # Load state_dict as is if no legacy issues\n",
    "        model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joints_to_vertices = JointsToVertices(device=DEVICE, mano_dir=cfg[\"data\"][\"mano_models_dir\"])\n",
    "\n",
    "# Setting the seed\n",
    "L.seed_everything(42, workers=True)\n",
    "\n",
    "print(f\"Loading data module...\")\n",
    "dataset_name = cfg[\"data\"].get(\"name\", \"dexycb\")\n",
    "if dataset_name == \"dexycb\":\n",
    "    dm = DexYCBDataModule(cfg[\"data\"])\n",
    "    views = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "    root_idx = 2\n",
    "elif dataset_name == \"mvhand\":\n",
    "    dm = MVHandDataModule(cfg[\"data\"])\n",
    "    views = [0, 1, 2, 3]\n",
    "    root_idx = 3\n",
    "elif dataset_name == \"ho3d\":\n",
    "    dm = HO3DDataModule(cfg[\"data\"])\n",
    "    views = [0, 1, 2, 3, 4]\n",
    "    root_idx = 0\n",
    "else:\n",
    "    print(f\"{dataset_name} dataset not found.\")\n",
    "    exit()\n",
    "\n",
    "if not VIEW_ONLY:\n",
    "    os.makedirs(f\"{OUTPUT_DIR}/{dataset_name}\", exist_ok=True)\n",
    "\n",
    "CHECKPOINT_PATH = SELECTED_EXP[\"weights\"]\n",
    "if CHECKPOINT_PATH:\n",
    "    print(\"\\nLoading model from checkpoint:\", CHECKPOINT_PATH)\n",
    "    model = Model(\n",
    "        train_params=cfg[\"train\"],\n",
    "        model_params=cfg[\"model\"],\n",
    "        data_params=cfg[\"data\"]\n",
    "    )\n",
    "    \n",
    "    # Check and fix mismatches if it's a legacy version\n",
    "    model = load_checkpoint_with_legacy_fix(CHECKPOINT_PATH, model, device=DEVICE)\n",
    "    model.eval()\n",
    "    model.to(DEVICE)\n",
    "\n",
    "else:\n",
    "    print(\"Checkpoint not found at:\", CHECKPOINT_PATH)\n",
    "    print(\"[Warn] Only drawing groundtruths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = dm.test_dataloader()  # dm.test_loader, dm.val_loader\n",
    "for i, sample in tqdm(enumerate(dataloader)):\n",
    "    visualizer = HandPoseVisualizer(sample, mano_dir=cfg[\"data\"][\"mano_models_dir\"])\n",
    "    if i % SKIP_EVERY != 0: continue\n",
    "    batch_size = sample[\"data\"][\"rgb\"].shape[0]\n",
    "\n",
    "    inputs = sample[\"data\"]\n",
    "    x = inputs[\"rgb\"]\n",
    "    bbox = inputs[\"bboxes\"]\n",
    "    cam_params = sample[\"cam_params\"]\n",
    "    \n",
    "    ########### inference ###############\n",
    "    if CHECKPOINT_PATH:\n",
    "        out = model.forward(x.to(DEVICE), bbox.to(DEVICE), cam_params)\n",
    "        out_joints_cam = out[\"joints_cam\"].detach().cpu() * 1000  # [b, 21, 3] in mm\n",
    "        pred_joints_crop_2d = out[\"joints_crop_img\"].detach().cpu()  # [b, v, 21, 2]\n",
    "        out_vertices = torch.stack([torch.from_numpy(joints_to_vertices(j.numpy())).float() for j in out_joints_cam])\n",
    "    ######################################\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        extr = sample[\"cam_params\"][\"extrinsic\"][b].float()  # [v, 4, 4]\n",
    "        extr_inv = torch.linalg.inv(extr.clone())  # [v, 4, 4]\n",
    "        intr = sample[\"cam_params\"][\"intrinsic\"][b].float()  # [v, 4]\n",
    "        Ks = create_intrinsics_matrix(intr)  # [v, 3, 3]\n",
    "\n",
    "        crops = sample[\"data\"][\"rgb\"][b]  # [v, 3, 256, 256]\n",
    "        # images = sample[\"data\"][\"full_rgb\"][b]  # [v, 3, h, w]\n",
    "        bbox = sample[\"data\"][\"bboxes\"][b]\n",
    "\n",
    "        #### ground truths\n",
    "        gt_joints_2d = sample[\"data\"][\"joints_img\"][b]  # [v, 21, 2]\n",
    "        gt_joints_crop_2d = sample[\"data\"][\"joints_crop_img\"][b]  # [v, 21, 2]\n",
    "        gt_joints_cam = sample[\"data\"][\"joints_cam\"][b]  # [21, 3]\n",
    "        gt_root = sample[\"data\"][\"root_joint\"][b]  # [1, 3]\n",
    "        gt_joints_cam_abs = gt_joints_cam + gt_root  # mm\n",
    "        gt_all_root = sample[\"data\"][\"all_root_joints\"][b]  # [v, 3]\n",
    "        gt_all_vertices = sample[\"data\"][\"all_vertices\"][b]  # [v, 778, 3]\n",
    "        gt_all_vertices_abs = gt_all_vertices + gt_all_root  # mm\n",
    "        gt_vertices =  sample[\"data\"][\"vertices\"][b] # [778, 3]\n",
    "        gt_vertices_abs = gt_vertices + gt_root        # mm\n",
    "        ##### Predictions\n",
    "        if CHECKPOINT_PATH:\n",
    "            pred_joints_crop_2d = pred_joints_crop_2d[b]  # [v, 21, 2]\n",
    "            pred_joints_cam = out_joints_cam[b]          # [21, 3]\n",
    "            pred_joints_cam_abs = pred_joints_cam + gt_root   # [21, 3]\n",
    "            pred_vertices = out_vertices[b]              # [778, 3]\n",
    "            pred_vertices_abs = pred_vertices + gt_root   # [778, 3]   \n",
    "                \n",
    "            pred_joints_crop_2d_proj = get_2d_joints_from_3d_joints(pred_joints_cam_abs.unsqueeze(0)/1000,\n",
    "                                                                root_idx,\n",
    "                                                                intr.unsqueeze(0),\n",
    "                                                                extr.unsqueeze(0)).squeeze(0)\n",
    "            # print(\"here:\", out[\"joints_cam\"].shape, pred_joints_cam.shape, pred_joints_crop_2d_proj.shape)\n",
    "            pred_joints_crop_2d_proj = batch_joints_img_to_cropped_joints(pred_joints_crop_2d_proj.view(-1, 21, 2), bbox.view(-1, 4))\n",
    "            # print(pred_joints_crop_2d.shape)\n",
    "\n",
    "        gt_combine, pred_combine = [], []\n",
    "        for v in views:\n",
    "            # print(v)\n",
    "            bb = bbox[v].numpy()\n",
    "            extrinsic = np.eye(4)\n",
    "            intrinsic = intr[v].numpy()\n",
    "            \n",
    "            bb_w, bb_h = bb[2] - bb[0], bb[3] - bb[1]\n",
    "            intrinsic[2], intrinsic[3] = intrinsic[2] - bb[0], intrinsic[3] - bb[1]\n",
    "            img = reverse_transform(crops[v], denormalize=True, IMAGENET_TRANSFORM=True)\n",
    "            img_orig = cv2.resize(img, (bb_w, bb_h))\n",
    "            \n",
    "            ############## Groundtruth Vis\n",
    "            gt_vis_imgs = []\n",
    "            # vertices = gt_all_vertices_abs.numpy()[v]\n",
    "            joints_crop = gt_joints_crop_2d.numpy()[v]\n",
    "\n",
    "            vertices = transform_joints_between_cameras(gt_vertices_abs/1000, extr[inputs[\"root_idx\"][0]], extr[v]).numpy()\n",
    "            vertices *= 1000\n",
    "\n",
    "            mesh_on_img, depth = visualizer.generate_mesh_from_verts(\n",
    "                                    vertices,\n",
    "                                    extrinsic,\n",
    "                                    intrinsic,\n",
    "                                    img_orig\n",
    "                                )\n",
    "            mesh_on_img = cv2.resize(mesh_on_img, (256, 256))\n",
    "            gt_vis_imgs.append(mesh_on_img)\n",
    "\n",
    "            # draw joints on image\n",
    "            joints_on_img = visualizer._draw_joints_on_image(img.copy(), joints_crop, point_size=6, edge_width=3)\n",
    "            gt_vis_imgs.append(joints_on_img)\n",
    "\n",
    "            if SAVE_GT:\n",
    "                if SAVE_INDIVIDUAL and not VIEW_ONLY:\n",
    "                    cv2.imwrite(f'{OUTPUT_DIR}/{dataset_name}/gt_{i}_{b}_{v}.png', np.hstack([img, joints_on_img, mesh_on_img])[:,:,::-1])\n",
    "            \n",
    "            gt_combine.append(np.vstack(gt_vis_imgs))\n",
    "\n",
    "            ############## Prediction Vis\n",
    "            if CHECKPOINT_PATH:\n",
    "                pred_vis_imgs = []\n",
    "                vertices = transform_joints_between_cameras(pred_vertices_abs/1000, extr[inputs[\"root_idx\"][0]], extr[v]).numpy()\n",
    "                vertices *= 1000\n",
    "                \n",
    "                # joints_crop = pred_joints_crop_2d.numpy()[v]\n",
    "                joints_crop = pred_joints_crop_2d_proj.numpy()[v]\n",
    "\n",
    "                mesh_on_img, depth = visualizer.generate_mesh_from_verts(\n",
    "                                        vertices,\n",
    "                                        extrinsic,\n",
    "                                        intrinsic,\n",
    "                                        img_orig\n",
    "                                    )\n",
    "                mesh_on_img = cv2.resize(mesh_on_img, (256, 256))\n",
    "                pred_vis_imgs.append(mesh_on_img)\n",
    "\n",
    "                # draw joints on image\n",
    "                joints_on_img = visualizer._draw_joints_on_image(img.copy(), joints_crop, point_size=6, edge_width=3)\n",
    "                pred_vis_imgs.append(joints_on_img)\n",
    "\n",
    "                if SAVE_INDIVIDUAL and not VIEW_ONLY:\n",
    "                    cv2.imwrite(f'{OUTPUT_DIR}/{dataset_name}/pred_{i}_{b}_{v}.png', np.hstack([img, joints_on_img, mesh_on_img])[:,:,::-1])\n",
    "                \n",
    "                pred_combine.append(np.vstack(pred_vis_imgs))\n",
    "\n",
    "        gt_vis = np.hstack(gt_combine)\n",
    "\n",
    "        if CHECKPOINT_PATH:\n",
    "            pred_vis = np.hstack(pred_combine)\n",
    "\n",
    "        if VIEW_ONLY:\n",
    "            if CHECKPOINT_PATH:\n",
    "                fig, axes = plt.subplots(2, 1, figsize=(5, 5))\n",
    "                for ax, vis_img, title in zip(axes, [pred_vis, gt_vis], [\"Prediction\", \"Groundtruth\"]):\n",
    "                    ax.imshow(vis_img)\n",
    "                    ax.set_title(title)\n",
    "                    ax.axis('off')\n",
    "            else:\n",
    "                plt.imshow(gt_vis)\n",
    "                plt.title(\"Groundtruth\")\n",
    "                plt.axis('off')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            if CHECKPOINT_PATH:\n",
    "                cv2.imwrite(f'{OUTPUT_DIR}/{dataset_name}/pred_{i}_{b}.png', pred_vis[:,:,::-1])\n",
    "            if SAVE_GT:\n",
    "                cv2.imwrite(f'{OUTPUT_DIR}/{dataset_name}/gt_{i}_{b}.png', gt_vis[:,:,::-1])\n",
    "        # break\n",
    "    \n",
    "    NUM_SAMPLES_TO_SAVE -= 1\n",
    "    if NUM_SAMPLES_TO_SAVE <= 0:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
